# Задание 1: Вычисление числа π методом Монте-Карло с использованием MPI

## Описание задания
Реализация параллельного алгоритма для вычисления числа π методом Монте-Карло с использованием технологии MPI. Программа равномерно распределяет точки между процессами и вычисляет отношение количества попаданий в единичную окружность к общему количеству точек.

## Метод Монте-Карло
- Генерируются случайные точки в квадрате 2×2
- Подсчитываются точки, попавшие в единичную окружность
- Вычисляется π ≈ 4 × (точки в окружности) / (все точки)

## Технические характеристики
- Язык программирования: C
- Технология параллелизации: MPI
- Компилятор: mpicc
- Оптимизация: -O3
- Количество процессов: 1, 2, 3, 4
- Размеры задач: 1M, 5M, 10M, 50M, 100M точек

## Результаты экспериментов

### Таблица производительности
| Количество точек | 1 процесс |  2 процесса   |  3 процесса   |  4 процесса   |
|------------------|-----------|---------------|---------------|---------------|
| 1,000,000        | 0.0282    | 0.0151 (1.87) | 0.0122 (2.31) | 0.0099 (2.84) |
| 5,000,000        | 0.1423    | 0.0718 (1.98) | 0.0481 (2.96) | 0.0432 (3.29) |
| 10,000,000       | 0.2856    | 0.1482 (1.93) | 0.1010 (2.83) | 0.0729 (3.92) |
| 50,000,000       | 1.4236    | 0.7243 (1.97) | 0.4723 (3.01) | 0.3712 (3.84) |
| 100,000,000      | 2.8549    | 1.4365 (1.99) | 0.9644 (2.96) | 0.7603 (3.75) |

*Формат: время в секундах (ускорение)*

## Анализ результатов

### 1. Время выполнения
- Прямая зависимость: время линейно зависит от количества точек
- Эффективность: на больших объемах данных (50M+ точек) программа демонстрирует почти линейное ускорение
- Минимальное время: 0.0099 сек (1M точек, 4 процесса)
- Максимальное время: 2.8549 сек (100M точек, 1 процесс)

### 2. Ускорение (Speedup)
- Маленькие задачи (1M точек): ускорение 2.84 на 4 процессах (эффективность 71.1%)
- Средние задачи (5M-10M точек): ускорение 3.29-3.92 (эффективность 82.3%-98.0%)
- Большие задачи (50M-100M точек): ускорение 3.75-3.84 (эффективность 93.9%-95.9%)

### 3. Эффективность параллелизации
- Высокая эффективность на больших объемах данных (>90%)
- Снижение эффективности на малых задачах из-за накладных расходов MPI
- Почти идеальное масштабирование при 50M+ точек

## Ключевые выводы

### Сильные стороны программы:
1. Отличное масштабирование на больших объемах данных
2. Высокая эффективность использования процессоров (до 98%)
3. Линейная зависимость времени от количества точек
4. Минимальные накладные расходы при больших N

### Ограничения:
1. Снижение эффективности на малых задачах
2. Зависимость от генератора случайных чисел
3. Коммуникационные затраты доминируют при малых N

### Рекомендации:
1. Использовать для задач от 10M точек для достижения хорошей эффективности
2. Для максимального ускорения использовать 3-4 процесса
3. При малых объемах данных выгоднее использовать последовательную версию

## Запуск программы

### Компиляция:
```bash
./run_experiments.sh
python3 plot_results.py
```

# Задание 2: MPI — умножение матрицы на вектор

## Описание задания
Реализация трех схем распараллеливания умножения матрицы на вектор с использованием MPI:
- row — разбиение по строкам
- col — разбиение по столбцам
- block — блочное (2D) разбиение

## Технические характеристики
- Язык программирования: C
- Технология параллелизации: MPI
- Компилятор: mpicc с оптимизацией -O3
- Количество процессов: 1, 2, 4
- Размеры матриц: 1000×1000, 2000×2000, 5000×5000, 10000×10000

## Результаты экспериментов

### Сводная таблица производительности
*Формат: время в секундах (ускорение)*

| Размер | Вид      | 1 процесс   | 2 процесса       | 4 процесса       |
|--------|----------|-------------|------------------|------------------|
| 1000   | row      | 0.000894    | 0.000445 (2.01)  | 0.000224 (3.99)  |
| 1000   | col      | 0.000897    | 0.000417 (2.15)  | 0.000204 (4.40)  |
| 1000   | block    | 0.000880    | 0.000453 (1.94)  | 0.000241 (3.66)  |
|        |          |             |                  |                  |
| 2000   | row      | 0.003815    | 0.001839 (2.07)  | 0.000948 (4.02)  |
| 2000   | col      | 0.003669    | 0.001834 (2.00)  | 0.001395 (2.63)  |
| 2000   | block    | 0.003696    | 0.001887 (1.96)  | 0.000987 (3.74)  |
|        |          |             |                  |                  |
| 5000   | row      | 0.023516    | 0.011552 (2.04)  | 0.006472 (3.63)  |
| 5000   | col      | 0.023361    | 0.012000 (1.95)  | 0.005855 (3.99)  |
| 5000   | block    | 0.023487    | 0.011644 (2.02)  | 0.006269 (3.75)  |
|        |          |             |                  |                  |
| 10000  | row      | 0.094250    | 0.047055 (2.00)  | 0.024482 (3.85)  |
| 10000  | col      | 0.094138    | 0.046410 (2.03)  | 0.025558 (3.68)  |
| 10000  | block    | 0.092249    | 0.047018 (1.96)  | 0.025471 (3.62)  |

## Детальный анализ эффективности

### Эффективность параллелизации (%):

| Размер | Вид      | 2 процесса | 4 процесса |
|--------|----------|------------|------------|
| 1000   | row      | 100.4%     | 99.8%      |
| 1000   | col      | 107.4%     | 109.9%     |
| 1000   | block    | 97.1%      | 91.5%      |
|        |          |            |            |
| 2000   | row      | 103.7%     | 100.6%     |
| 2000   | col      | 100.0%     | 65.8%      |
| 2000   | block    | 98.0%      | 93.6%      |
|        |          |            |            |
| 5000   | row      | 101.8%     | 90.8%      |
| 5000   | col      | 97.3%      | 99.7%      |
| 5000   | block    | 100.9%     | 93.7%      |
|        |          |            |            |
| 10000  | row      | 100.1%     | 96.2%      |
| 10000  | col      | 101.4%     | 92.1%      |
| 10000  | block    | 98.1%      | 90.5%      |

## Ключевые выводы

### Лучшие результаты по ускорению:
1. col-схема на 1000 элементах: 4.40× ускорение (превышение идеального!)
2. row-схема на 2000 элементах: 4.02× ускорение
3. col-схема на 5000 элементах: 3.99× ускорение

### Анализ по схемам:

#### Row-схема:
- Стабильное ускорение на всех размерах
- Лучшая эффективность на средних и больших матрицах
- Минимальные коммуникационные затраты

#### Col-схема:
- Лучшее ускорение на малых матрицах (до 109.9% эффективности!)
- Сильное падение эффективности на 2000×2000 (65.8%)
- Нестабильная производительность

#### Block-схема:
- Хороший баланс на больших матрицах
- Худшие показатели на малых размерах
- Стабильное масштабирование

### Наблюдения:
1. Сверхлинейное ускорение (эффективность >100%) наблюдается из-за кэширования и оптимизаций компилятора
2. Col-схема требует суммирования частичных результатов, что увеличивает коммуникацию
3. Row-схема демонстрирует наиболее предсказуемое поведение
4. Эффективность снижается с ростом числа процессов из-за накладных расходов MPI

## Рекомендации по использованию

### Для практического применения:
- Малые матрицы (≤2000): использовать col-схему для максимального ускорения
- Средние и большие матрицы (≥5000): использовать row-схему для стабильной производительности
- Распределенные системы: рассмотреть block-схему для лучшего масштабирования

### Для дальнейших исследований:
- Тестирование на большем количестве процессов
- Анализ коммуникационных паттернов
- Сравнение с OpenMP и гибридными подходами

## Запуск программы

### Компиляция:
```bash
mpicc -O3 -std=c99 -o matvec_mpi matvec_mpi.c
./run_experiments.sh
python3 plot_results.py
```

# Задание 3: Умножение матриц по алгоритму Кэннона с использованием MPI

## Описание задания
Реализация параллельного алгоритма умножения матриц по алгоритму Кэннона с использованием технологии MPI. Алгоритм Кэннона использует блочное разбиение матриц и циклические сдвиги для эффективного распараллеливания операции матричного умножения на квадратной сетке процессов.

## Алгоритм Кэннона
- Матрицы разбиваются на блоки, распределяемые по процессам в декартовой топологии
- Выполняется начальное выравнивание блоков матриц A и B
- Проводится последовательность сдвигов и локальных умножений
- Собирается итоговая матрица в корневом процессе

## Технические характеристики
- Язык программирования: C
- Технология параллелизации: MPI
- Компилятор: mpicc с оптимизацией -O3
- Количество процессов: 1, 4 (квадраты целых чисел)
- Размеры матриц: 300×300, 600×600, 900×900, 1500×1500, 2000×2000

## Результаты экспериментов

### Таблица производительности
*Формат: время в секундах (ускорение)*

| Размер матрицы | 1 процесс    | 4 процессов      |
|----------------|--------------|------------------|
| 300×300        | 0.0294       | 0.0081 (3.62)    |
| 600×600        | 0.2353       | 0.0681 (3.46)    |
| 900×900        | 0.8574       | 0.2265 (3.79)    |
| 1500×1500      | 10.0463      | 1.4872 (6.76)    |
| 2000×2000      | 52.3219      | 5.6132 (9.32)    |

## Детальный анализ эффективности

### Эффективность параллелизации (%):

| Размер матрицы | Эффективность на 4 процессах |
|----------------|-------------------------------|
| 300×300        | 90.6%                         |
| 600×600        | 86.4%                         |
| 900×900        | 94.7%                         |
| 1500×1500      | 168.9%                        |
| 2000×2000      | 233.0%                        |

## Ключевые выводы

### Наблюдения:
1. Сверхлинейное ускорение на больших матрицах (1500×1500 и больше) - эффективность превышает 100%
2. Отличная масштабируемость - ускорение растет с увеличением размера задачи
3. Высокая эффективность даже на малых матрицах (>85% на 300×300)

### Анализ производительности:

#### Малые матрицы (300×300 - 900×900):
- Стабильное ускорение 3.5-3.8× на 4 процессах
- Эффективность 86-95%
- Накладные расходы MPI заметны, но не критичны

#### Большие матрицы (1500×1500 - 2000×2000):
- Сверхлинейное ускорение 6.76-9.32×
- Эффективность 169-233%
- Оптимальное использование кэшей процессоров

### Преимущества алгоритма Кэннона:
1. Минимальные коммуникации - только соседние процессы обмениваются данными
2. Хороший баланс нагрузки - равномерное распределение работы
3. Эффективное использование памяти - работа с блоками уменьшает промахи кэша

### Ограничения:
1. Требует квадратного числа процессов (1, 4, 9, 16, ...)
2. Размер матрицы должен быть кратен размеру сетки
3. Сложность реализации по сравнению с простыми схемами

## Рекомендации по использованию

### Для практического применения:
- Использовать для больших матриц (≥1000×1000) для достижения максимальной эффективности
- Выбирать размер матрицы, кратный размеру сетки процессов
- Для максимального ускорения использовать 4, 9 или 16 процессов

### Оптимальные сценарии:
- Научные вычисления с большими плотными матрицами
- Обработка изображений и компьютерное зрение
- Машинное обучение (линейная алгебра)

## Запуск программы

### Компиляция:
```bash
mpicc -O3 -std=c99 -o matrix_mult_cannon matrix_mult_cannon.c -lm
./run_matrix_experiments.sh
python3 plot_matrix_results.py
```
